"""
Memory Assistant - Phase 1
ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®ã¿ï¼ˆOllamaé€£æºï¼‰
"""
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, StreamingResponse
from pydantic import BaseModel
import ollama
from pathlib import Path

app = FastAPI(title="Memory Assistant", version="1.0.0")

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡
app.mount("/static", StaticFiles(directory="static"), name="static")


class ChatRequest(BaseModel):
    message: str
    model: str = "llama3.1:8b"


class ChatResponse(BaseModel):
    response: str
    model: str


@app.get("/", response_class=HTMLResponse)
async def root():
    """ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã§index.htmlã‚’è¿”ã™"""
    index_path = Path("static/index.html")
    if index_path.exists():
        return index_path.read_text(encoding="utf-8")
    return "<h1>Memory Assistant</h1><p>index.html ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“</p>"


@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        # OllamaãŒå‹•ã„ã¦ã„ã‚‹ã‹ç¢ºèª
        models = ollama.list()
        return {
            "status": "healthy",
            "ollama": "connected",
            "available_models": [m.get("name", m.get("model", "unknown")) for m in models.get("models", [])]
        }
    except Exception as e:
        return {
            "status": "degraded",
            "ollama": "disconnected",
            "error": str(e)
        }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    Ollamaã‚’ä½¿ã£ã¦LLMã¨å¯¾è©±
    """
    try:
        # Ollamaã§ãƒãƒ£ãƒƒãƒˆ
        response = ollama.chat(
            model=request.model,
            messages=[
                {
                    "role": "system",
                    "content": "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ãã€å°‘ã—å¿˜ã‚Œã£ã½ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                },
                {
                    "role": "user",
                    "content": request.message
                }
            ]
        )

        assistant_message = response["message"]["content"]

        return ChatResponse(
            response=assistant_message,
            model=request.model
        )

    except ollama.ResponseError as e:
        raise HTTPException(
            status_code=500,
            detail=f"Ollama error: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Unexpected error: {str(e)}"
        )


@app.get("/models")
async def list_models():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§"""
    try:
        models = ollama.list()
        return {
            "models": [
                {
                    "name": m.get("name", m.get("model", "unknown")),
                    "size": m.get("size", 0),
                    "modified_at": m.get("modified_at", "")
                }
                for m in models.get("models", [])
            ]
        }
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to list models: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Memory Assistant - Phase 1")
    print("ğŸ“ http://localhost:8000")
    print("ğŸ’¡ Ollama must be running: ollama serve")
    uvicorn.run(app, host="0.0.0.0", port=8000)
